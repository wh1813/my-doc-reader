import os
import time
import logging
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def get_driver():
    options = uc.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    # å¼ºåˆ¶æŒ‡å®šç‰ˆæœ¬ 144
    driver = uc.Chrome(options=options, version_main=144, use_subprocess=True)
    return driver

# ==================== Book118 é€»è¾‘ (ä¿æŒä¸å˜) ====================
def crawl_book118(driver):
    urls = []
    base_domain = "https://max.book118.com"
    cookie_str = os.environ.get("COOKIE_BOOK118")
    
    if not cookie_str:
        logging.warning("âš ï¸ [Book118] æœªé…ç½® COOKIE_BOOK118ï¼Œè·³è¿‡")
        return []

    try:
        logging.info(">>> [Book118] å¼€å§‹æŠ“å–...")
        driver.get("https://max.book118.com/")
        driver.delete_all_cookies()
        for item in cookie_str.split(';'):
            if '=' in item:
                k, v = item.strip().split('=', 1)
                driver.add_cookie({'name': k.strip(), 'value': v.strip()})
        
        target_url = "https://max.book118.com/user_center_v1/doc/index/index.html#audited"
        driver.get(target_url)
        time.sleep(5)

        last_page_first_link = None
        
        for page in range(1, 101):
            logging.info(f"   [Book118] åˆ†æç¬¬ {page} é¡µ...")
            try: WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "tr")))
            except: pass

            rows = driver.find_elements(By.TAG_NAME, "tr")
            current_page_links = []
            
            for row in rows:
                try:
                    try:
                        views_text = row.find_element(By.CSS_SELECTOR, "td.col-click").text.strip()
                        if "ä¸‡" in views_text: views = float(views_text.replace("ä¸‡", "")) * 10000
                        else: views = int(views_text)
                    except: continue

                    if views < 15:
                        link_elm = row.find_element(By.CSS_SELECTOR, "td.col-title a")
                        link = link_elm.get_attribute("href")
                        if link and "http" not in link: link = base_domain + link
                        if link: current_page_links.append(link)
                except: continue

            if not current_page_links:
                logging.info("   æœ¬é¡µæ— ç¬¦åˆæ¡ä»¶çš„ä½çƒ­åº¦é“¾æ¥")
                if not rows: break # è¿è¡Œéƒ½æ²¡æ‰¾åˆ°ï¼Œè¯´æ˜å¯èƒ½å‡ºé”™äº†æˆ–åˆ°åº•äº†
            
            # é˜²é‡
            if current_page_links and current_page_links[0] == last_page_first_link:
                logging.info("ğŸ›‘ æ£€æµ‹åˆ°é‡å¤é¡µé¢ï¼Œåœæ­¢")
                break
            if current_page_links: last_page_first_link = current_page_links[0]
            
            urls.extend(current_page_links)
            logging.info(f"      -> æ•è· {len(current_page_links)} ä¸ªä½çƒ­åº¦é“¾æ¥")

            try:
                next_btn = driver.find_element(By.XPATH, "//a[contains(text(), 'ä¸‹ä¸€é¡µ')]")
                href = next_btn.get_attribute("href")
                if not href or "javascript" in href: break
                driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(4)
            except: break

    except Exception as e:
        logging.error(f"âŒ [Book118] å‡ºé”™: {e}")
    
    return urls

# ==================== RenrenDoc é€»è¾‘ (æ·±åº¦ä¿®å¤ç‰ˆ) ====================
def crawl_renrendoc_single(driver, cookie_name, cookie_value):
    urls = []
    if not cookie_value: return []
    
    logging.info(f">>> [{cookie_name}] å¼€å§‹æŠ“å–...")
    try:
        driver.get("https://www.renrendoc.com/")
        driver.delete_all_cookies()
        for item in cookie_value.split(';'):
            if '=' in item:
                k, v = item.strip().split('=', 1)
                driver.add_cookie({'name': k.strip(), 'value': v.strip()})

        driver.get("https://www.renrendoc.com/renrendoc_v1/MCBookList/published.html")
        time.sleep(5)

        last_page_links_set = set()

        for page in range(1, 101):
            logging.info(f"   [{cookie_name}] åˆ†æç¬¬ {page} é¡µ...")
            
            # 1. æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼è¡Œ (TR)
            rows = driver.find_elements(By.TAG_NAME, "tr")
            if not rows:
                logging.warning("   âš ï¸ æœªæ‰¾åˆ°è¡¨æ ¼è¡Œï¼Œå°è¯•æŸ¥æ‰¾åˆ—è¡¨å®¹å™¨...")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šæœ‰äº›é¡µé¢å¯èƒ½æ˜¯ div åˆ—è¡¨ï¼Œè¿™é‡Œä¿ç•™æ‰©å……ç©ºé—´
            
            current_page_found = []
            
            for row in rows:
                try:
                    # 2. åœ¨æ¯ä¸€è¡Œä¸­å¯»æ‰¾ "æ•°å­—/æ•°å­—" æ ¼å¼çš„å•å…ƒæ ¼
                    # è·å–è¯¥è¡Œæ‰€æœ‰å•å…ƒæ ¼
                    cols = row.find_elements(By.TAG_NAME, "td")
                    
                    is_low_view = False
                    link_found = None
                    
                    for col in cols:
                        text = col.text.strip()
                        
                        # --- æ ¸å¿ƒè¯†åˆ«é€»è¾‘ ---
                        # æ£€æŸ¥æ˜¯å¦åŒ…å« "/" ä¸”è¢«åˆ†å‰²çš„ä¸¤éƒ¨åˆ†éƒ½æ˜¯æ•°å­—
                        if "/" in text:
                            parts = text.split("/")
                            if len(parts) == 2 and parts[0].isdigit():
                                views = int(parts[0]) # æå–æ–œæ å·¦è¾¹çš„æµè§ˆé‡
                                
                                if views < 15:
                                    is_low_view = True
                                else:
                                    # å¦‚æœæµè§ˆé‡ >= 15ï¼Œè¿™è¡Œç›´æ¥è·³è¿‡ï¼Œä¸ç”¨æ‰¾é“¾æ¥äº†
                                    break 
                        
                        # åŒæ—¶åœ¨è¿™ä¸ªå¾ªç¯é‡Œæ‰¾é“¾æ¥ (é€šå¸¸åœ¨æ ‡é¢˜åˆ—)
                        # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬æ‰¾è¯¥è¡Œå†…æ‰€æœ‰å«æœ‰ "renrendoc.com/p-" çš„é“¾æ¥
                        if not link_found:
                            try:
                                # åªæ‰¾è¿™ä¸€ä¸ªå•å…ƒæ ¼é‡Œçš„é“¾æ¥
                                sub_links = col.find_elements(By.TAG_NAME, "a")
                                for sub_link in sub_links:
                                    href = sub_link.get_attribute("href")
                                    if href and ("renrendoc.com/p-" in href or "renrendoc.com/paper/" in href):
                                        link_found = href
                                        break
                            except: pass

                    # 3. åªæœ‰å½“ï¼šæ˜¯ä½æµè§ˆé‡ AND æ‰¾åˆ°äº†é“¾æ¥ï¼Œæ‰åŠ å…¥åˆ—è¡¨
                    if is_low_view and link_found:
                        current_page_found.append(link_found)
                        
                except Exception as row_e:
                    continue
            
            # === é˜²é‡ä¸ç¿»é¡µ ===
            current_set = set(current_page_found)
            if not current_page_found and not rows:
                logging.info("   æœ¬é¡µæ— æ•°æ®ï¼Œåœæ­¢")
                break
                
            if current_set and current_set == last_page_links_set:
                logging.info(f"ğŸ›‘ [{cookie_name}] é¡µé¢é‡å¤ï¼Œåœæ­¢")
                break
                
            last_page_links_set = current_set
            urls.extend(current_page_found)
            logging.info(f"      -> æ•è· {len(current_page_found)} ä¸ªä½çƒ­åº¦é“¾æ¥")
            
            try:
                next_btn = driver.find_element(By.XPATH, "//a[contains(@class, 'paginator') and contains(text(), 'ä¸‹ä¸€é¡µ')]")
                driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(4)
            except: 
                logging.info(f"   [{cookie_name}] ç¿»é¡µç»“æŸ")
                break
                
    except Exception as e:
        logging.error(f"âŒ [{cookie_name}] å‡ºé”™: {e}")
    
    return urls

def crawl_renrendoc_all(driver):
    all_renren_urls = []
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»ï¼Œå¦‚æœæœ¬åœ°æµ‹è¯•æ²¡é…ç½®ç¯å¢ƒå˜é‡ï¼Œå¯ä»¥æ‰‹åŠ¨å¡«
    renren_keys = ["COOKIE_RENREN1", "COOKIE_RENREN2"]
    
    for key in renren_keys:
        val = os.environ.get(key)
        if val:
            all_renren_urls.extend(crawl_renrendoc_single(driver, key, val))
        else:
            logging.info(f"â„¹ï¸ {key} æœªé…ç½®ï¼Œè·³è¿‡")
            
    return all_renren_urls

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨æ™ºèƒ½ç­›é€‰çˆ¬è™« (ä»…æŠ“å–é˜…è¯»é‡ < 15)...")
    
    driver = get_driver()
    if driver:
        final_urls = []
        
        # 1. Book118
        final_urls.extend(crawl_book118(driver))
        time.sleep(3)
        
        # 2. Renren
        final_urls.extend(crawl_renrendoc_all(driver))
        
        # 3. ä¿å­˜
        # æ³¨æ„ï¼šè¿™é‡Œæ˜¯è¦†ç›–å†™å…¥ ('w')ï¼Œè¿™æ„å‘³ç€æ¯æ¬¡ç”Ÿæˆéƒ½æ˜¯å…¨æ–°çš„â€œå¾…å¤„ç†åå•â€
        final_urls = list(set(final_urls))
        
        if final_urls:
            with open("urls.txt", "w", encoding="utf-8") as f:
                for url in final_urls:
                    f.write(url + "\n")
            logging.info(f"ğŸ‰ æŠ“å–å®Œæˆï¼å…±ç”Ÿæˆ {len(final_urls)} ä¸ªã€ä½çƒ­åº¦ã€‘é“¾æ¥")
            logging.info("ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³ urls.txtï¼Œè¯·æ¨é€åˆ° GitHub")
        else:
            logging.warning("âš ï¸ æœ¬æ¬¡æœªæŠ“å–åˆ°ä»»ä½• < 15 é˜…è¯»é‡çš„é“¾æ¥ (å¯èƒ½æ˜¯éƒ½åˆ·ä¸Šå»äº†ï¼Œæˆ–è€…Cookieå¤±æ•ˆ)")
            
        try: driver.quit()
        except: pass
