import os
import time
import logging
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By

# è®¾ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def get_driver():
    """å¯åŠ¨æµè§ˆå™¨é…ç½®"""
    options = uc.ChromeOptions()
    # åœ¨ GitHub Actions æˆ–æœåŠ¡å™¨åå°è¿è¡Œæ—¶å¿…é¡»å¼€å¯ headless
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    
    # ä¿æŒä¸ main.py ä¸€è‡´çš„é©±åŠ¨ç‰ˆæœ¬é€»è¾‘
    driver = uc.Chrome(options=options, version_main=144, use_subprocess=True)
    return driver

def crawl_book118_user_center(driver):
    target_urls = []
    base_domain = "https://max.book118.com" # ç”¨äºè¡¥å…¨ç›¸å¯¹è·¯å¾„
    
    try:
        logging.info(">>> [çˆ¬è™«] æ­£åœ¨åˆå§‹åŒ– Book118...")
        driver.get("https://max.book118.com/")
        
        # 1. æ³¨å…¥ Cookie (ä»ç¯å¢ƒå˜é‡è·å–)
        cookie_str = os.environ.get("COOKIE_BOOK118")
        if not cookie_str:
            logging.error("âŒ æœªæ£€æµ‹åˆ° Cookieï¼Œè¯·æ£€æŸ¥ GitHub Secrets (COOKIE_BOOK118)ï¼")
            return []
            
        logging.info("æ­£åœ¨æ³¨å…¥ç™»å½•å‡­è¯...")
        driver.delete_all_cookies()
        for item in cookie_str.split(';'):
            if '=' in item:
                key_val = item.strip().split('=', 1)
                if len(key_val) == 2:
                    driver.add_cookie({'name': key_val[0], 'value': key_val[1]})
        
        # 2. è·³è½¬åˆ°æ–‡æ¡£ç®¡ç†åå°
        user_center_url = "https://max.book118.com/user_center/doc_manage" 
        logging.info(f"æ­£åœ¨è·³è½¬åå°: {user_center_url}")
        driver.get(user_center_url)
        time.sleep(5) # ç­‰å¾…é¡µé¢åŠ è½½
        
        # ç®€å•æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
        if "login" in driver.current_url:
            logging.error("âŒ ç™»å½•å¤±è´¥ï¼ŒCookie å¯èƒ½å·²è¿‡æœŸï¼Œè¯·é‡æ–°è·å–ï¼")
            return []

        # 3. å¾ªç¯çˆ¬å–å‰ 5 é¡µ (å¯æ ¹æ®éœ€è¦ä¿®æ”¹èŒƒå›´)
        for page in range(1, 6):
            logging.info(f"--- æ­£åœ¨åˆ†æç¬¬ {page} é¡µ ---")
            
            rows = driver.find_elements(By.TAG_NAME, "tr")
            found_count = 0
            
            for row in rows:
                try:
                    # --- A. è·å–ç‚¹å‡»é‡ ---
                    try:
                        views_element = row.find_element(By.CSS_SELECTOR, "td.col-click")
                        views_text = views_element.text.strip()
                    except:
                        continue # è·³è¿‡éæ–‡æ¡£è¡Œ
                    
                    # ç»Ÿä¸€è½¬æ¢ä¸ºæ•°å­—
                    views = 0
                    if "ä¸‡" in views_text:
                        views = float(views_text.replace("ä¸‡", "")) * 10000
                    elif views_text.isdigit():
                        views = int(views_text)
                    else:
                        continue 

                    # --- B. ç­›é€‰æ¡ä»¶ï¼šç‚¹å‡»é‡ < 15 ---
                    if views < 15:
                        # --- C. è·å–é“¾æ¥ ---
                        title_elem = row.find_element(By.CSS_SELECTOR, "td.col-title a.title")
                        link_href = title_elem.get_attribute("href")
                        doc_title = title_elem.get_attribute("title") or "æ— æ ‡é¢˜"
                        
                        # è¡¥å…¨é“¾æ¥
                        if link_href and not link_href.startswith("http"):
                            link_href = base_domain + link_href
                        
                        if link_href:
                            target_urls.append(link_href)
                            logging.info(f"âœ… æ•è·: [{views}æ¬¡] {doc_title}")
                            found_count += 1
                            
                except Exception:
                    continue 
            
            if found_count == 0:
                logging.info("æœ¬é¡µæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ä½é¢‘æ–‡æ¡£")
            
            # --- D. ç¿»é¡µ ---
            if page < 5:
                next_url = f"{user_center_url}?page={page+1}"
                driver.get(next_url)
                time.sleep(3)

    except Exception as e:
        logging.error(f"âŒ è¿è¡Œå‡ºé”™: {e}")

    return target_urls

def save_urls(urls):
    if not urls:
        logging.info("æœ¬æ¬¡æ²¡æœ‰æŠ“å–åˆ°é“¾æ¥ï¼Œä¸æ›´æ–°æ–‡ä»¶ã€‚")
        return
    
    logging.info(f"æ­£åœ¨ä¿å­˜ {len(urls)} ä¸ªé“¾æ¥åˆ° urls.txt...")
    # è¦†ç›–å†™å…¥ urls.txt
    with open("urls.txt", "w", encoding="utf-8") as f:
        for url in urls:
            f.write(url + "\n")
    logging.info("ğŸ‰ ä¿å­˜æˆåŠŸï¼")

if __name__ == "__main__":
    driver = get_driver()
    if driver:
        urls = crawl_book118_user_center(driver)
        save_urls(urls)
        try:
            driver.quit()
        except:
            pass