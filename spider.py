import os
import time
import logging
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def get_driver():
    options = uc.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    driver = uc.Chrome(options=options, version_main=144, use_subprocess=True)
    return driver

# ==================== Book118 é€»è¾‘ ====================
def crawl_book118(driver):
    urls = []
    base_domain = "https://max.book118.com"
    cookie_str = os.environ.get("COOKIE_BOOK118")
    
    if not cookie_str:
        logging.warning("âš ï¸ [Book118] æœªé…ç½® COOKIE_BOOK118ï¼Œè·³è¿‡")
        return []

    try:
        logging.info(">>> [Book118] å¼€å§‹æŠ“å–...")
        driver.get("https://max.book118.com/")
        driver.delete_all_cookies()
        for item in cookie_str.split(';'):
            if '=' in item:
                k, v = item.strip().split('=', 1)
                driver.add_cookie({'name': k.strip(), 'value': v.strip()})
        
        target_url = "https://max.book118.com/user_center_v1/doc/index/index.html#audited"
        driver.get(target_url)
        time.sleep(5)

        # ç”¨äºé˜²é‡ï¼šè®°å½•ä¸Šä¸€é¡µæ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªé“¾æ¥ï¼Œå¦‚æœå½“å‰é¡µç¬¬ä¸€ä¸ªé“¾æ¥å’Œä¸Šä¸€é¡µä¸€æ ·ï¼Œè¯´æ˜æ²¡ç¿»è¿‡å»
        last_page_first_link = None
        
        # æ‰©å¤§ç¿»é¡µæ•°åˆ° 100 (çº¦2000æ¡)
        for page in range(1, 101):
            logging.info(f"   æ­£åœ¨åˆ†æç¬¬ {page} é¡µ...")
            try:
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "tr")))
            except: pass

            rows = driver.find_elements(By.TAG_NAME, "tr")
            current_page_links = []
            
            for row in rows:
                try:
                    # è·å–ç‚¹å‡»é‡
                    try:
                        views_text = row.find_element(By.CSS_SELECTOR, "td.col-click").text.strip()
                        if "ä¸‡" in views_text:
                            views = float(views_text.replace("ä¸‡", "")) * 10000
                        else:
                            views = int(views_text)
                    except: continue

                    if views < 15:
                        link_elm = row.find_element(By.CSS_SELECTOR, "td.col-title a")
                        link = link_elm.get_attribute("href")
                        if link and "http" not in link: link = base_domain + link
                        if link: 
                            current_page_links.append(link)
                except: continue

            # === é˜²é‡/ç¿»é¡µå¤±è´¥æ£€æµ‹ ===
            if not current_page_links:
                logging.info("âš ï¸ æœ¬é¡µæœªæ‰¾åˆ°æœ‰æ•ˆé“¾æ¥ï¼Œåœæ­¢ç¿»é¡µ")
                break
                
            current_first_link = current_page_links[0]
            if current_first_link == last_page_first_link:
                logging.info("ğŸ›‘ æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼ˆç¿»é¡µæœªç”Ÿæ•ˆæˆ–å·²è¾¾é™åˆ¶ï¼‰ï¼Œåœæ­¢æŠ“å–")
                break
            
            last_page_first_link = current_first_link
            urls.extend(current_page_links)
            logging.info(f"   ç¬¬ {page} é¡µæ•è· {len(current_page_links)} ä¸ªé“¾æ¥")

            # æ‰§è¡Œç¿»é¡µ
            try:
                next_btn = driver.find_element(By.XPATH, "//a[contains(text(), 'ä¸‹ä¸€é¡µ')]")
                href = next_btn.get_attribute("href")
                # å¦‚æœ href ä¸ºç©ºæˆ–è€…æ˜¯ javascript:; è¯´æ˜æ˜¯æœ€åä¸€é¡µ
                if not href or "javascript" in href: 
                    logging.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
                    
                driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(4)
            except: 
                logging.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œç»“æŸ")
                break

    except Exception as e:
        logging.error(f"âŒ [Book118] å‡ºé”™: {e}")
    
    return urls

# ==================== RenrenDoc é€»è¾‘ (å¤šè´¦å· + é˜²é‡) ====================
def crawl_renrendoc_single(driver, cookie_name, cookie_value):
    urls = []
    if not cookie_value: return []
    
    logging.info(f">>> [{cookie_name}] å¼€å§‹æŠ“å–...")
    try:
        driver.get("https://www.renrendoc.com/")
        driver.delete_all_cookies()
        for item in cookie_value.split(';'):
            if '=' in item:
                k, v = item.strip().split('=', 1)
                driver.add_cookie({'name': k.strip(), 'value': v.strip()})

        driver.get("https://www.renrendoc.com/renrendoc_v1/MCBookList/published.html")
        time.sleep(5)

        last_page_links_set = set()

        for page in range(1, 101): # æ‰©å¤§èŒƒå›´
            logging.info(f"   [{cookie_name}] åˆ†æç¬¬ {page} é¡µ...")
            
            links = driver.find_elements(By.TAG_NAME, "a")
            current_page_found = []
            
            for link in links:
                try:
                    href = link.get_attribute("href")
                    if href and "renrendoc.com/p-" in href:
                        current_page_found.append(href)
                except: continue
            
            # === é˜²é‡æ£€æµ‹ ===
            current_set = set(current_page_found)
            if not current_set:
                logging.info("æœ¬é¡µæ— æ•°æ®ï¼Œåœæ­¢")
                break
                
            # å¦‚æœå½“å‰é¡µæ‰¾åˆ°çš„æ‰€æœ‰é“¾æ¥ï¼Œéƒ½å·²ç»åœ¨ä¸Šä¸€é¡µå‡ºç°è¿‡ï¼ˆè¯´æ˜é¡µé¢æ²¡å˜ï¼‰
            # æˆ–è€…å½“å‰é¡µå†…å®¹å’Œä¸Šä¸€é¡µå®Œå…¨ä¸€æ ·
            if current_set == last_page_links_set:
                logging.info(f"ğŸ›‘ [{cookie_name}] æ£€æµ‹åˆ°é‡å¤é¡µé¢ï¼ˆå·²è¾¾é™åˆ¶ï¼‰ï¼Œåœæ­¢")
                break
                
            last_page_links_set = current_set
            urls.extend(current_page_found)
            
            # ç¿»é¡µ
            try:
                next_btn = driver.find_element(By.XPATH, "//a[contains(@class, 'paginator') and contains(text(), 'ä¸‹ä¸€é¡µ')]")
                driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(4)
            except: 
                logging.info(f"   [{cookie_name}] ç¿»é¡µç»“æŸ")
                break
                
    except Exception as e:
        logging.error(f"âŒ [{cookie_name}] å‡ºé”™: {e}")
    
    return urls

def crawl_renrendoc_all(driver):
    all_renren_urls = []
    renren_keys = ["COOKIE_RENREN1", "COOKIE_RENREN2"]
    
    for key in renren_keys:
        val = os.environ.get(key)
        if val:
            all_renren_urls.extend(crawl_renrendoc_single(driver, key, val))
        else:
            logging.info(f"â„¹ï¸ {key} æœªé…ç½®ï¼Œè·³è¿‡")
            
    return all_renren_urls

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    driver = get_driver()
    if driver:
        final_urls = []
        
        # 1. Book118
        final_urls.extend(crawl_book118(driver))
        
        # 2. Renren (Renren1 + Renren2)
        final_urls.extend(crawl_renrendoc_all(driver))
        
        # 3. å»é‡å¹¶ä¿å­˜
        final_urls = list(set(final_urls))
        if final_urls:
            with open("urls.txt", "w", encoding="utf-8") as f:
                for url in final_urls:
                    f.write(url + "\n")
            logging.info(f"ğŸ‰ æŠ“å–å®Œæˆï¼å…±æ›´æ–° {len(final_urls)} ä¸ªé“¾æ¥")
        else:
            logging.info("âš ï¸ æœ¬æ¬¡æœªæŠ“å–åˆ°ä»»ä½•é“¾æ¥")
            
        try: driver.quit()
        except: pass
