import os
import time
import logging
import random
import sys
import shutil
import threading
import subprocess
import json
import requests
import urllib.parse
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from http.server import HTTPServer, BaseHTTPRequestHandler

# ================= é…ç½®åŒºåŸŸ =================
# ä»“åº“åœ°å€
REPO_PATH = "wh1813/my-doc-reader"

# è¿œç¨‹æ–‡ä»¶åœ°å€
REMOTE_URLS_PATH = f"https://raw.githubusercontent.com/{REPO_PATH}/main/urls.txt"
REMOTE_XRAY_PATH = f"https://raw.githubusercontent.com/{REPO_PATH}/main/xray.txt"

# çˆ¬è™«å®šæ—¶å™¨ï¼šæ¯éš”å¤šå°‘ç§’è¿è¡Œä¸€æ¬¡çˆ¬è™« (é»˜è®¤ 12 å°æ—¶ = 43200 ç§’)
# å¦‚æœä½ å¸Œæœ›åªä¾èµ– GitHub Actions æ›´æ–°ï¼Œå¯ä»¥æŠŠè¿™ä¸ªè®¾å¾—éå¸¸å¤§
SPIDER_INTERVAL = 43200 

# é‡å¯é—´éš” (æ¯è®¿é—®å¤šå°‘ä¸ªç½‘é¡µé‡å¯æµè§ˆå™¨)
RESTART_INTERVAL = 50
# ===========================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

# å…¨å±€å˜é‡ï¼šè®°å½•ä¸Šæ¬¡çˆ¬è™«è¿è¡Œæ—¶é—´
last_spider_time = 0

# --- æ¨¡å—: è°ƒç”¨æœ¬åœ°çˆ¬è™« ---
def run_spider_task():
    """è¿è¡Œæœ¬åœ° spider.py å¹¶æ›´æ–° urls.txt"""
    global last_spider_time
    logging.info("ğŸ•·ï¸ >>> [çˆ¬è™«ä»»åŠ¡] æ­£åœ¨å¯åŠ¨æœ¬åœ°çˆ¬è™«...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ Cookie ç¯å¢ƒå˜é‡ (å¦åˆ™çˆ¬è™«è·‘äº†ä¹Ÿæ²¡ç”¨)
    if not os.environ.get("COOKIE_BOOK118") and not os.environ.get("COOKIE_RENREN1"):
        logging.warning("âš ï¸ æœªæ£€æµ‹åˆ° Cookie ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æœ¬åœ°çˆ¬å– (å°†å°è¯•ä½¿ç”¨è¿œç¨‹ urls.txt)")
        return

    try:
        # è°ƒç”¨ spider.py
        result = subprocess.run(["python", "spider.py"], capture_output=True, text=True)
        if result.returncode == 0:
            logging.info("âœ… [çˆ¬è™«ä»»åŠ¡] æ‰§è¡ŒæˆåŠŸï¼Œurls.txt å·²æ›´æ–°")
            # æ‰“å°çˆ¬è™«çš„éƒ¨åˆ†è¾“å‡ºä»¥ä¾¿è°ƒè¯•
            print(result.stderr) 
        else:
            logging.error(f"âŒ [çˆ¬è™«ä»»åŠ¡] æ‰§è¡Œå¤±è´¥: {result.stderr}")
            
        last_spider_time = time.time()
        
    except Exception as e:
        logging.error(f"âŒ [çˆ¬è™«ä»»åŠ¡] è°ƒç”¨å¼‚å¸¸: {e}")

# --- æ¨¡å—1: VLESS é“¾æ¥è§£æå™¨ ---
def parse_vless(url):
    try:
        if not url.startswith("vless://"): return None
        main_part = url.split("://")[1].split("?")[0].split("#")[0]
        query_part = url.split("?")[1].split("#")[0] if "?" in url else ""
        user_info, host_port = main_part.split("@")
        host, port = host_port.split(":")
        params = dict(urllib.parse.parse_qsl(query_part))
        return {
            "uuid": user_info, "address": host, "port": int(port),
            "type": params.get("type", "tcp"), "security": params.get("security", "none"),
            "sni": params.get("sni", ""), "path": params.get("path", "/"),
            "host": params.get("host", ""), "fp": params.get("fp", "")
        }
    except: return None

# --- æ¨¡å—2: ä»£ç†æœåŠ¡ç®¡ç† (Xray) ---
def check_proxy_connectivity():
    try:
        proxies = {"http": "http://127.0.0.1:10808", "https": "http://127.0.0.1:10808"}
        r = requests.get("https://www.baidu.com", proxies=proxies, timeout=5)
        return r.status_code == 200
    except: return False

def start_xray_with_node(node_url):
    node = parse_vless(node_url)
    if not node: return False
    config = {
        "log": {"loglevel": "error"},
        "inbounds": [{"port": 10808, "listen": "127.0.0.1", "protocol": "http", "settings": {"udp": True}}],
        "outbounds": [{
            "protocol": "vless",
            "settings": {"vnext": [{"address": node["address"], "port": node["port"], "users": [{"id": node["uuid"], "encryption": "none"}]}]},
            "streamSettings": {
                "network": node["type"], "security": node["security"],
                "tlsSettings": {"serverName": node["sni"], "fingerprint": node["fp"]} if node["security"] == "tls" else None,
                "wsSettings": {"path": node["path"], "headers": {"Host": node["host"]}} if node["type"] == "ws" else None
            }
        }]
    }
    with open("config.json", "w") as f: json.dump(config, f)
    subprocess.run("pkill -9 -f xray", shell=True, stderr=subprocess.DEVNULL)
    time.sleep(1)
    try:
        subprocess.Popen(["xray", "-c", "config.json"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(2)
        if check_proxy_connectivity():
            logging.info(f"    -> [èŠ‚ç‚¹åˆ‡æ¢æˆåŠŸ] ç›®æ ‡: {node['address']}")
            return True
        else: return False
    except: return False

def rotate_proxy():
    if not os.path.exists("xray.txt"): return False
    with open("xray.txt", "r") as f:
        nodes = [line.strip() for line in f if line.strip() and not line.startswith("#")]
    if not nodes: return False
    random.shuffle(nodes)
    for node_url in nodes:
        if start_xray_with_node(node_url): return True
    return False

# --- æ¨¡å—3: è‡ªåŠ¨æ›´æ–° ---
def update_remote_files():
    # åªæœ‰å½“æœ¬åœ°æ²¡æœ‰ urls.txt æˆ–è€…æ–‡ä»¶ä¸ºç©ºæ—¶ï¼Œæ‰å¼ºåˆ¶ä»è¿œç¨‹æ‹‰å–
    # é¿å…è¦†ç›–äº†æœ¬åœ°çˆ¬è™«åˆšæŠ“åˆ°çš„æ–°é²œæ•°æ®
    should_update_urls = True
    if os.path.exists("urls.txt") and os.path.getsize("urls.txt") > 0:
        # ç®€å•ç­–ç•¥ï¼šå¦‚æœæœ¬åœ°æœ‰æ•°æ®ï¼Œæš‚æ—¶ä¸ä»è¿œç¨‹è¦†ç›–ï¼Œé™¤éä½ æƒ³åˆå¹¶
        # è¿™é‡Œä¸ºäº†é…åˆâ€œæœ¬åœ°çˆ¬è™«ä¼˜å…ˆâ€ï¼Œæˆ‘ä»¬ä»…æ›´æ–° xray.txt
        should_update_urls = False 

    try:
        if should_update_urls:
            r = requests.get(REMOTE_URLS_PATH, timeout=10)
            if r.status_code == 200:
                with open("urls.txt", "w", encoding="utf-8") as f: f.write(r.text)
                logging.info("âœ… urls.txt ä»è¿œç¨‹æ›´æ–°æˆåŠŸ")
        
        # Xray èŠ‚ç‚¹åˆ—è¡¨æ€»æ˜¯æ›´æ–°
        r = requests.get(REMOTE_XRAY_PATH, timeout=10)
        if r.status_code == 200:
            with open("xray.txt", "w", encoding="utf-8") as f: f.write(r.text)
            logging.info("âœ… xray.txt ä»è¿œç¨‹æ›´æ–°æˆåŠŸ")
    except: pass

# --- æ¨¡å—4: æµè§ˆå™¨é…ç½® ---
def force_kill_chrome():
    subprocess.run("pkill -9 -f chrome", shell=True, stderr=subprocess.DEVNULL)
    subprocess.run("pkill -9 -f undetected_chromedriver", shell=True, stderr=subprocess.DEVNULL)
    subprocess.run("rm -rf /tmp/.org.chromium.*", shell=True, stderr=subprocess.DEVNULL)

def get_driver():
    force_kill_chrome()
    data_dir = "/tmp/chrome_user_data"
    if os.path.exists(data_dir): shutil.rmtree(data_dir, ignore_errors=True)
    options = uc.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument(f"--user-data-dir={data_dir}")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--proxy-server=http://127.0.0.1:10808")
    try:
        driver = uc.Chrome(options=options, version_main=144, use_subprocess=True, headless=True)
        driver.set_page_load_timeout(60)
        return driver
    except:
        force_kill_chrome()
        return None

# --- ä¸»é€»è¾‘ ---
def run_automation():
    global last_spider_time
    
    # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡Œçˆ¬è™« (å¯åŠ¨æ—¶å¿…è·‘ï¼Œä¹‹åæŒ‰ SPIDER_INTERVAL è·‘)
    current_time = time.time()
    if last_spider_time == 0 or (current_time - last_spider_time > SPIDER_INTERVAL):
        run_spider_task()

    # 2. å¦‚æœæœ¬åœ°çˆ¬è™«æ²¡è·‘æˆï¼Œå°è¯•ä» GitHub æ‹‰å–ä¿åº•
    if not os.path.exists("urls.txt") or os.path.getsize("urls.txt") == 0:
        update_remote_files()

    # 3. æ£€æŸ¥ä»£ç†
    if subprocess.call("pgrep -f xray > /dev/null", shell=True) != 0:
        if not rotate_proxy(): return 

    # 4. è¯»å– URL
    if not os.path.exists("urls.txt"): return
    with open("urls.txt", "r") as f: urls = [l.strip() for l in f if l.strip()]
    if not urls: 
        logging.warning("âš ï¸ urls.txt ä¸ºç©ºï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡å¾ªç¯...")
        return

    driver = get_driver()
    if not driver: return
    logging.info(f">>> ä»»åŠ¡å¼€å§‹ï¼Œæœ¬è½®å…± {len(urls)} ä¸ªé“¾æ¥")

    for index, url in enumerate(urls, 1):
        try:
            if not url.startswith('http'): url = 'https://' + url
            if index % RESTART_INTERVAL == 0:
                try: driver.quit()
                except: pass
                if not rotate_proxy(): break 
                driver = get_driver()
                if not driver: break

            logging.info(f"[{index}/{len(urls)}] è®¿é—®: {url}")
            driver.get(url)
            sleep_time = random.uniform(5, 8)
            time.sleep(sleep_time)
            logging.info(f"    -> æˆåŠŸ (åœç•™ {sleep_time:.1f}s)")
        except:
            try: driver.quit()
            except: pass
            rotate_proxy()
            driver = get_driver()
            if not driver: break
    try: driver.quit()
    except: pass
    force_kill_chrome()

# --- ä¿æ´» Web Server ---
class HealthCheckHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.wfile.write(b"Alive")
    def log_message(self, format, *args): pass

if __name__ == "__main__":
    threading.Thread(target=HTTPServer(('0.0.0.0', 80), HealthCheckHandler).serve_forever, daemon=True).start()
    
    # åˆå§‹åŒ–ä»£ç†
    update_remote_files() # å…ˆæ‹‰å– xray.txt
    if not rotate_proxy(): time.sleep(10)
    
    while True:
        try: run_automation()
        except Exception as e: 
            logging.error(f"ä¸»å¾ªç¯å¼‚å¸¸: {e}")
        
        # ä¼‘æ¯ 10 åˆ†é’Ÿåè¿›å…¥ä¸‹ä¸€è½® (å¦‚æœæ˜¯çˆ¬è™«åˆšè·‘å®Œï¼Œè¿™é‡Œä¹Ÿä¼šä¼‘æ¯ï¼Œé˜²æ­¢é¢‘ç¹è¯·æ±‚)
        logging.info("ğŸ’¤ ä¼‘æ¯ 10 åˆ†é’Ÿ...")
        time.sleep(600)
