import os
import time
import logging
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def get_driver():
    options = uc.ChromeOptions()
    # ç”Ÿäº§ç¯å¢ƒ/GitHub Actions è¯·åŠ¡å¿…å¼€å¯ headless
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    
    driver = uc.Chrome(options=options, version_main=144, use_subprocess=True)
    return driver

# ==================== Book118 çˆ¬è™« (åŸºäºå·²éªŒè¯çš„ HTML) ====================
def crawl_book118(driver):
    urls = []
    base_domain = "https://max.book118.com"
    logging.info(">>> [Book118] å¼€å§‹æŠ“å–...")

    try:
        # 1. ç™»å½•
        driver.get("https://max.book118.com/")
        cookie_str = os.environ.get("COOKIE_BOOK118")
        if not cookie_str:
            logging.error("âŒ [Book118] æœªé…ç½® Cookieï¼")
            return []
        
        driver.delete_all_cookies()
        for item in cookie_str.split(';'):
            if '=' in item:
                k, v = item.strip().split('=', 1)
                driver.add_cookie({'name': k.strip(), 'value': v.strip()})
        
        # 2. è®¿é—®æ–°ç‰ˆåå°
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ä½ æä¾›çš„æ–°ç‰ˆåå°åœ°å€
        start_url = "https://max.book118.com/user_center_v1/doc/index/index.html#audited"
        driver.get(start_url)
        time.sleep(5) 

        # 3. å¾ªç¯ç¿»é¡µ
        for page in range(1, 6): # çˆ¬å–å‰ 5 é¡µ
            logging.info(f"--- [Book118] åˆ†æç¬¬ {page} é¡µ ---")
            
            # ç­‰å¾…åˆ—è¡¨åŠ è½½ (é˜²æ­¢ç½‘ç»œæ…¢å¯¼è‡´æŠ“ç©º)
            try:
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "tr")))
            except:
                logging.warning("ç­‰å¾…è¡¨æ ¼è¶…æ—¶æˆ–é¡µé¢ä¸ºç©º")

            # A. åˆ†æå½“å‰é¡µæ•°æ®
            rows = driver.find_elements(By.TAG_NAME, "tr")
            found_count = 0
            for row in rows:
                try:
                    # è·å–ç‚¹å‡»é‡ (åŸºäºä¹‹å‰çš„ HTML: td.col-click)
                    try:
                        views_elm = row.find_element(By.CSS_SELECTOR, "td.col-click")
                        views_text = views_elm.text.strip()
                        if "ä¸‡" in views_text:
                            views = float(views_text.replace("ä¸‡", "")) * 10000
                        else:
                            views = int(views_text)
                    except:
                        continue # æ²¡æ‰¾åˆ°ç‚¹å‡»é‡ï¼Œå¯èƒ½æ˜¯è¡¨å¤´
                    
                    # ç­›é€‰ç‚¹å‡»é‡ < 15
                    if views < 15:
                        # è·å–é“¾æ¥ (åŸºäºä¹‹å‰çš„ HTML: td.col-title a)
                        title_elm = row.find_element(By.CSS_SELECTOR, "td.col-title a")
                        link = title_elm.get_attribute("href")
                        
                        # è¡¥å…¨é“¾æ¥
                        if link and "http" not in link: 
                            link = base_domain + link
                        
                        if link:
                            urls.append(link)
                            logging.info(f"âœ… [Book118] æ•è·: {link}")
                            found_count += 1
                except:
                    continue

            # B. æ‰§è¡Œç¿»é¡µ (åŸºäºä½ æä¾›çš„ HTML)
            # HTML: <a href="/user_center_v1/...">ä¸‹ä¸€é¡µ</a>
            try:
                # ä½¿ç”¨ XPath ç²¾å‡†æŸ¥æ‰¾æ–‡å­—ä¸º"ä¸‹ä¸€é¡µ"çš„é“¾æ¥
                next_btn = driver.find_element(By.XPATH, "//a[contains(text(), 'ä¸‹ä¸€é¡µ')]")
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ (å¦‚æœ href æ˜¯å½“å‰é¡µæˆ–è€… javascript:; å¯èƒ½å°±æ˜¯æ²¡äº†)
                href = next_btn.get_attribute("href")
                if not href or "javascript" in href:
                    logging.info("æ²¡æœ‰ä¸‹ä¸€é¡µäº†")
                    break
                    
                logging.info("æ­£åœ¨ç‚¹å‡»ä¸‹ä¸€é¡µ...")
                # ç›´æ¥ç‚¹å‡»æ¯” get(href) æ›´ç¨³ï¼Œå› ä¸ºå®ƒèƒ½ä¿æŒ Session ä¸Šä¸‹æ–‡
                driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(5) # ç­‰å¾…æ–°é¡µé¢åŠ è½½
            except Exception as e:
                logging.info(f"æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œçˆ¬å–ç»“æŸã€‚")
                break

    except Exception as e:
        logging.error(f"âŒ [Book118] å¼‚å¸¸: {e}")

    return urls

# ==================== RenrenDoc çˆ¬è™« (åŸºäºå·²éªŒè¯çš„ç¿»é¡µ) ====================
def crawl_renrendoc(driver):
    urls = []
    logging.info(">>> [RenrenDoc] å¼€å§‹æŠ“å–...")

    try:
        # 1. ç™»å½•
        driver.get("https://www.renrendoc.com/")
        cookie_str = os.environ.get("COOKIE_RENRENDOC")
        if not cookie_str:
            logging.warning("âš ï¸ [RenrenDoc] æœªé…ç½® Cookieï¼Œè·³è¿‡ã€‚")
            return []
        
        driver.delete_all_cookies()
        for item in cookie_str.split(';'):
            if '=' in item:
                k, v = item.strip().split('=', 1)
                driver.add_cookie({'name': k.strip(), 'value': v.strip()})

        # 2. è®¿é—®åå°
        start_url = "https://www.renrendoc.com/renrendoc_v1/MCBookList/published.html"
        driver.get(start_url)
        time.sleep(5)

        # 3. å¾ªç¯ç¿»é¡µ
        for page in range(1, 6):
            logging.info(f"--- [RenrenDoc] åˆ†æç¬¬ {page} é¡µ ---")
            
            # A. åˆ†æå½“å‰é¡µæ•°æ® (æš‚æ—¶ä½¿ç”¨é€šç”¨æŠ“å–ï¼Œå› ç¼ºå°‘åˆ—è¡¨ HTML)
            # ç­–ç•¥ï¼šæŠ“å–é¡µé¢ä¸»è¦å†…å®¹åŒºçš„æ‰€æœ‰æ–‡æ¡£é“¾æ¥
            # äººäººæ–‡æ¡£çš„é“¾æ¥ç‰¹å¾é€šå¸¸åŒ…å« /p-
            links = driver.find_elements(By.TAG_NAME, "a")
            found_count = 0
            for link in links:
                try:
                    href = link.get_attribute("href")
                    # ç®€å•ç­›é€‰ï¼šå¿…é¡»åŒ…å« renrendoc.com ä¸”åŒ…å«æ–‡æ¡£ ID ç‰¹å¾
                    if href and "renrendoc.com/p-" in href:
                        urls.append(href)
                        found_count += 1
                        # logging.info(f"âœ… [RenrenDoc] æ•è·: {href}") # é“¾æ¥å¤ªå¤šå¯ä»¥å…³æ‰æ—¥å¿—
                except:
                    continue
            logging.info(f"    æœ¬é¡µæå–åˆ° {found_count} ä¸ªæ½œåœ¨æ–‡æ¡£é“¾æ¥")

            # B. æ‰§è¡Œç¿»é¡µ (åŸºäºä½ æä¾›çš„ HTML)
            # HTML: <a class="paginator" href="...?page=7">ä¸‹ä¸€é¡µ</a>
            try:
                # ä½¿ç”¨ CSS é€‰æ‹©å™¨å®šä½ class="paginator" ä¸”æ–‡å­—åŒ…å«"ä¸‹ä¸€é¡µ"
                # è¿™é‡Œç”¨ XPATH æœ€ç¨³ï¼Œå› ä¸º paginator å¯èƒ½æœ‰å¤šä¸ª(ä¸Šä¸€é¡µ/é¡µç )
                next_btn = driver.find_element(By.XPATH, "//a[contains(@class, 'paginator') and contains(text(), 'ä¸‹ä¸€é¡µ')]")
                
                logging.info("ç‚¹å‡»ä¸‹ä¸€é¡µ...")
                driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(4)
            except:
                logging.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œçˆ¬å–ç»“æŸã€‚")
                break

    except Exception as e:
        logging.error(f"âŒ [RenrenDoc] å¼‚å¸¸: {e}")
    
    return urls

# ==================== ä¸»ç¨‹åº ====================
def save_urls(urls):
    if not urls: return
    urls = list(set(urls)) # å»é‡
    with open("urls.txt", "w", encoding="utf-8") as f:
        for url in urls:
            f.write(url + "\n")
    logging.info(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼urls.txt å·²æ›´æ–°ï¼Œå…± {len(urls)} ä¸ªé“¾æ¥ã€‚")

if __name__ == "__main__":
    driver = get_driver()
    if driver:
        all_urls = []
        all_urls.extend(crawl_book118(driver))
        all_urls.extend(crawl_renrendoc(driver))
        save_urls(all_urls)
        try: driver.quit()
        except: pass
